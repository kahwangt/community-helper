{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to extract trending topics and Top 10 hashtags from Twitter API\n",
    "This script will requests tweets from Twitter API for each of the 31 LGAs based on their localities/suburbs which has been stored in a csv file <b>filtered_locality.csv</b> together with their latitudes and longitudes. The script was written in a way in which requests to Twitter for each LGA will be made through a concatenation of all suburbs under a particular LGA (e.g. 'caulfield OR glenhuntly OR Murrumbeena OR Carnegie OR Bentleigh OR Gardenvale OR Elsternwick OR BrightonEast OR McKinnon OR Ormond') for Glen Eira City Council. 1000 tweets were requested within 10 miles distance of the latitudes and longitudes. Note that due to the restriction of Twitter Public API, only tweets tweeted in the last 7 days will be received.\n",
    "\n",
    "After which, text cleaning was done to the tweets message to get the trending topics/keywords in each LGA based on their frequency and the top 10 hashtags in each LGA based on their frequency as well. The two resulting dataframes are the stored into csv files for further usage in Microsoft Power BI visualisation.\n",
    "\n",
    "### Output Documents\n",
    "1. <b>TRENDING.csv</b> - contains 100 trending topics/keywords for each of the 31 LGA, their frequency count and the corresponding LGA\n",
    "2. <b>HASHTAGS.csv</b> - contains top 10 hashtags for each of the 31 LGA, their frequency count and the corresponding LGA\n",
    "\n",
    "### Note\n",
    "- Manual input required in <b>Setup Twitter Connection</b> with the `consumer_key`, `consumer_secret`, `access_token` and `access_secret` obtained from Twitter Developers Website with individual account\n",
    "- Ensure that `filtered_locality.csv` is in the same directory as this script file in order to ensure the script can read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Twitter Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:twitteR':\n",
      "\n",
      "    id, location\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Using direct authentication\"\n"
     ]
    }
   ],
   "source": [
    "# import necessary library for Twitter API, data manipulation and text cleaning\n",
    "library(\"twitteR\")\n",
    "library(\"ROAuth\")\n",
    "library(\"dplyr\")\n",
    "library(\"tidytext\")\n",
    "library(\"stringr\")\n",
    "\n",
    "# Set up Twitter Connection\n",
    "consumer_key <- ''                     ############### INPUT TWITTER DEVELOPER API TOKENS HERE ###############\n",
    "consumer_secret<- ''\n",
    "access_token <- ''\n",
    "access_secret <- ''\n",
    "setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Extract tweets and populate trending topics/hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n",
      "Joining, by = \"word\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Done!\"\n"
     ]
    }
   ],
   "source": [
    "# read in filtered_locality which contains the 31 LGAs and their locality/suburbs\n",
    "locality = read.csv(\"./filtered_locality.csv\", header = TRUE)\n",
    "\n",
    "options(warn=-1)\n",
    "# set flag for storage of tweets\n",
    "flag1 = 0\n",
    "# set flag for storage of top 10 topics\n",
    "flag2 = 0\n",
    "# set flag for storage of hashtags\n",
    "flag3 = 0\n",
    "# loop through the 31 LGA/Municipality Name\n",
    "for (region in as.character(unique(locality$Municipality.Name))){\n",
    "    # filter the locality dataframe to this region\n",
    "    temp = locality %>% filter(Municipality.Name==region)\n",
    "    # we need to split the suburbs search to 20 each time due to Twitter limitations\n",
    "    if (nrow(temp)<=20){\n",
    "        # initialize empty string\n",
    "        search_string = \"\"\n",
    "        # store all suburbs under this locality into a string for Twitter search\n",
    "        for (suburb in temp[,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string == \"\"){\n",
    "                search_string = paste(search_string, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string = paste(search_string, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        # store latitude, longitude and set distance to search to 10 miles\n",
    "        geocode = paste(temp$latitude[2], temp$longitude[2], '10mi', sep = \",\")\n",
    "        # request data from Twitter API using suburb/locality name, geocode above, 1000 results and language english\n",
    "        tweets <- searchTwitter(search_string, n=1000, geocode=geocode, lang = 'en')\n",
    "        # convert tweets to df\n",
    "        df <- twListToDF(tweets)\n",
    "    }\n",
    "    # if there's more than 20 but less than 40 suburbs\n",
    "    else if ((nrow(temp)>20) && (nrow(temp)<=40)){\n",
    "        search_string_1 = \"\"\n",
    "        search_string_2 = \"\"\n",
    "        for (suburb in temp[1:20,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_1 == \"\"){\n",
    "                search_string_1 = paste(search_string_1, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_1 = paste(search_string_1, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        for (suburb in temp[21:nrow(temp),1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_2 == \"\"){\n",
    "                search_string_2 = paste(search_string_2, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_2 = paste(search_string_2, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        # store latitude, longitude and set distance to search to 10 miles\n",
    "        geocode = paste(temp$latitude[2], temp$longitude[2], '10mi', sep = \",\")\n",
    "        # request data from Twitter API using suburb/locality name, geocode above, 1000 results and language english\n",
    "        tweets1 <- searchTwitter(search_string_1, n=1000, geocode=geocode, lang = 'en')\n",
    "        df <- twListToDF(tweets1)\n",
    "        tweets2 <- searchTwitter(search_string_2, n=1000, geocode=geocode, lang = 'en')\n",
    "        if (length(tweets2)!=0){\n",
    "            df <- rbind(df, twListToDF(tweets2))\n",
    "            }\n",
    "    }\n",
    "    # if there's more than 40 but less than 60 suburbs\n",
    "    else if ((nrow(temp)>40) && (nrow(temp)<=60)){\n",
    "        search_string_1 = \"\"\n",
    "        search_string_2 = \"\"\n",
    "        search_string_3 = \"\"\n",
    "        for (suburb in temp[1:20,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_1 == \"\"){\n",
    "                search_string_1 = paste(search_string_1, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_1 = paste(search_string_1, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        for (suburb in temp[21:40,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_2 == \"\"){\n",
    "                search_string_2 = paste(search_string_2, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_2 = paste(search_string_2, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        for (suburb in temp[41:nrow(temp),1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_3 == \"\"){\n",
    "                search_string_3 = paste(search_string_3, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_3 = paste(search_string_3, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        # store latitude, longitude and set distance to search to 10 miles\n",
    "        geocode = paste(temp$latitude[2], temp$longitude[2], '10mi', sep = \",\")\n",
    "        # request data from Twitter API using suburb/locality name, geocode above, 1000 results and language english\n",
    "        tweets1 <- searchTwitter(search_string_1, n=1000, geocode=geocode, lang = 'en')\n",
    "        tweets2 <- searchTwitter(search_string_2, n=1000, geocode=geocode, lang = 'en')\n",
    "        tweets3 <- searchTwitter(search_string_3, n=1000, geocode=geocode, lang = 'en')\n",
    "        # convert tweets to df\n",
    "        df <- twListToDF(tweets1)\n",
    "        if (length(tweets2)!=0){\n",
    "            df <- rbind(df, twListToDF(tweets2))\n",
    "            }\n",
    "        if (length(tweets3)!=0){\n",
    "            df <- rbind(df, twListToDF(tweets3))\n",
    "            }\n",
    "    }\n",
    "    # if there are more than 60 suburbs\n",
    "    else if ((nrow(temp)>60)){\n",
    "        search_string_1 = \"\"\n",
    "        search_string_2 = \"\"\n",
    "        search_string_3 = \"\"\n",
    "        search_string_4 = \"\"\n",
    "        for (suburb in temp[1:20,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_1 == \"\"){\n",
    "                search_string_1 = paste(search_string_1, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_1 = paste(search_string_1, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        for (suburb in temp[21:40,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_2 == \"\"){\n",
    "                search_string_2 = paste(search_string_2, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_2 = paste(search_string_2, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        for (suburb in temp[41:60,1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_3 == \"\"){\n",
    "                search_string_3 = paste(search_string_3, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_3 = paste(search_string_3, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        for (suburb in temp[61:nrow(temp),1]){\n",
    "            temp_string = gsub(\" \", \"\", suburb)\n",
    "            if (search_string_4 == \"\"){\n",
    "                search_string_4 = paste(search_string_4, temp_string)\n",
    "            }\n",
    "            else{\n",
    "                search_string_4 = paste(search_string_4, temp_string, sep = ' OR ')  \n",
    "            }\n",
    "        }\n",
    "        # store latitude, longitude and set distance to search to 10 miles\n",
    "        geocode = paste(temp$latitude[2], temp$longitude[2], '10mi', sep = \",\")\n",
    "        # request data from Twitter API using suburb/locality name, geocode above, 1000 results and language english\n",
    "        tweets1 <- searchTwitter(search_string_1, n=1000, geocode=geocode, lang = 'en')\n",
    "        tweets2 <- searchTwitter(search_string_2, n=1000, geocode=geocode, lang = 'en')\n",
    "        tweets3 <- searchTwitter(search_string_3, n=1000, geocode=geocode, lang = 'en')\n",
    "        tweets4 <- searchTwitter(search_string_4, n=1000, geocode=geocode, lang = 'en')\n",
    "        # convert tweets to df\n",
    "        df <- twListToDF(tweets1)\n",
    "        df <- rbind(df, twListToDF(tweets2))\n",
    "        df <- rbind(df, twListToDF(tweets3))\n",
    "        if (length(tweets4)!=0){\n",
    "            df <- rbind(df, twListToDF(tweets4))\n",
    "            }\n",
    "    }\n",
    "    # remove duplicates based on id (i.e. column 8)\n",
    "    df = df[!duplicated(df[,8]),]\n",
    "    # initialize empty vector to store hashtags\n",
    "    hashvec = vector()\n",
    "    # extract all hashtags in tweets text using regex\n",
    "    tags_list=str_extract_all(df$text,\"#[a-zA-Z0-9]{1,}\")\n",
    "    # loop through the extracted hashtags and add to hashvec\n",
    "    for (i in 1:length(df$text)){\n",
    "        if (length(tags_list[[i]]!=0)){\n",
    "            for (j in 1:length(tags_list[[i]])){\n",
    "            hashvec <- c(hashvec, tags_list[[i]][j])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # convert all hashtags to lowercase\n",
    "    hashvec <- tolower(hashvec)\n",
    "    # if this is the first LGA, create a hashtags dataframe\n",
    "    if (flag3==0){\n",
    "        HASHTAGS = as.data.frame(hashvec)\n",
    "        # get the count\n",
    "        HASHTAGS = HASHTAGS %>%dplyr::count(hashvec, sort = TRUE)\n",
    "        # keep only the top 10 hashtags\n",
    "        HASHTAGS <- HASHTAGS[1:10,]\n",
    "        # store the LGA name\n",
    "        HASHTAGS['LGA'] = region\n",
    "        flag3 <- 1\n",
    "    }\n",
    "    # if not, create a temp_hash and rbind to hashtags dataframe\n",
    "    else{\n",
    "        temp_hash = as.data.frame(hashvec)\n",
    "        # get the count\n",
    "        temp_hash = temp_hash %>%dplyr::count(hashvec, sort = TRUE)\n",
    "        # keep only the top 10 hashtags\n",
    "        temp_hash <- temp_hash[1:10,]\n",
    "        # store the LGA name\n",
    "        temp_hash['LGA'] = region\n",
    "        # perform rbind to hashtags\n",
    "        HASHTAGS <- rbind(HASHTAGS, temp_hash)\n",
    "    }\n",
    "    \n",
    "    # text cleaning\n",
    "    df$text=as.character(df$text)\n",
    "    df$text <- gsub(\"\\\\$\", \"\", df$text) \n",
    "    df$text <- gsub(\"@\\\\w+\", \"\", df$text)\n",
    "    df$text <- gsub(\"[[:punct:]]\",\"\", df$text)\n",
    "    df$text <- gsub(\"http\\\\w+\", \"\", df$text)\n",
    "    df$text <- gsub(\"[ |\\t]{2,}\", \"\", df$text)\n",
    "    df$text <- gsub(\"^ \", \"\", df$text)\n",
    "    df$text <- gsub(\" $\", \"\", df$text)\n",
    "    df$text <- gsub(\"RT\",\"\",df$text)\n",
    "    df$text <- gsub(\"href\", \"\", df$text)\n",
    "    # split the text into individual tokens\n",
    "    text_df <- data_frame(text = df$text) %>%\n",
    "      unnest_tokens(word, text)\n",
    "    # if this is the first LGA/municipality name processed, do\n",
    "    if (flag2==0){\n",
    "        # remove stopwords, count the words frequency, sort it by descending order\n",
    "        TRENDING <- text_df %>%\n",
    "          anti_join(stop_words) %>%\n",
    "          dplyr::count(word, sort = TRUE)\n",
    "        # keep only the top 100 words\n",
    "        TRENDING <- TRENDING[1:100,]\n",
    "        # create column LGA to store the LGA/municipality name\n",
    "        TRENDING['LGA'] <- region\n",
    "        flag2 <- 1\n",
    "    }\n",
    "    # rbind the dataframe to df if this is the second locality/suburb onwards\n",
    "    else{\n",
    "        # remove stopwords, count the words frequency, sort it by descending order\n",
    "        temp_output <- text_df %>%\n",
    "          anti_join(stop_words) %>%\n",
    "          dplyr::count(word, sort = TRUE)\n",
    "        # keep only the top 100 words\n",
    "        temp_output <- temp_output[1:100,]\n",
    "        # create column LGA to store the LGA/municipality name\n",
    "        temp_output['LGA'] <- region\n",
    "        # rbind temp_output to output\n",
    "        TRENDING <- rbind(TRENDING, temp_output)\n",
    "    }\n",
    "}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the 2 dataframes into .csv files\n",
    "write.csv(TRENDING, './TRENDING.csv',row.names = FALSE)\n",
    "write.csv(HASHTAGS, './HASHTAGS.csv',row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------- END OF SCRIPT --------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
