{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to extract Eventbrite API data\n",
    "This script will populate all localities/suburbs under each LGA, extract future events and past events (28 days before) data from Eventbrite API.<br><br>\n",
    "Section 1 will populate all localities/suburbs under each of the 31 LGAs using the two datasets, <a href=\"http://www.corra.com.au/australian-postcode-location-data/\">Latitude and Longitude of localities in Australia</a> and <a href=\"https://discover.data.vic.gov.au/dataset/victorian-electors-by-locality-postcode-and-electorates\">List of localities/suburbs of each LGA in Victoria</a> for further use in Section 2 and Section 3.<br><br>\n",
    "\n",
    "Section 2 will extract past 28 days events data in each LGA from Eventbrite API from the defined `start_date` and `end_date` and categorised based on their culture type through text classification using the events' name and description. The final dataframe will be stored into a .csv file for further analysis with Twitter API and visualisation in Microsoft Power BI.<br><br>\n",
    "\n",
    "Section 3 will extract all future events from the defined `end_date` and categorised based on their culture type through text classification using the events' name and description. The final dataframe will be stored into a .csv file for further analysis with Twitter API and visualisation in Microsoft Power BI.<br><br>\n",
    "\n",
    "\n",
    "### Output Documents\n",
    "1. <b>eventbritedatapastevents.csv</b> - contains past 28 days events of each community with their name, description, id, start_date and culture category\n",
    "2. <b>eventbritedatafutureevents.csv</b> - contains future events of each community with their name, id and culture category\n",
    "\n",
    "### Note\n",
    "- Manual input of dates required in <b>Set Date</b> section in the format `YYYY-MM-DDTHH:MM:SS` \n",
    "- Manual input required in <b>Section 1</b> if extending locations from current 31 LGAs\n",
    "- Manual input required in <b>Section 2</b> and <b>Section 3</b> to key in EventBrite API Access Code in the format `Bearer xxxxxxxxxxxxxxxxxxxx`, where `xxxxxxxxxxxxxxxxxxxx` should be replaced\n",
    "- Ensure that `Australian_Post_Codes_Lat_Lon.csv` and `LocalityFinder.xls` are in the same directory as this script file in order to ensure the script can be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set start_date and end_date\n",
    "start_date = '2019-09-10T00:00:01'\n",
    "end_date = '2019-10-10T23:59:59'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Populating localities/suburbs under each LGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data manipulation pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# read in lat_lon data\n",
    "lat_lon = pd.read_csv(\"./Australian_Post_Codes_Lat_Lon.csv\")\n",
    "# filter lat_lon to just Victoria state\n",
    "lat_lon = lat_lon.loc[lat_lon['state']=='VIC', ['postcode','suburb','lat','lon']]\n",
    "# read in locality data\n",
    "locality = pd.read_excel(\"./LocalityFinder.xls\")\n",
    "# set headers as 2nd row as file is not cleaned initially\n",
    "headers = locality.iloc[1]\n",
    "# remove 1st and 2nd row\n",
    "locality = locality[2:]\n",
    "# set the right column headers\n",
    "locality.columns = headers\n",
    "# remove unnecessary columns\n",
    "locality = locality.iloc[:,0:3]\n",
    "# store the 31 LGA accorinding to the format in locality data (e.g. Yarra City Council, Yarra Ranges Shire Council)\n",
    "LGA = ['Hobsons Bay City Council', 'Maribyrnong City Council', 'Melbourne City Council', 'Yarra City Council',\\\n",
    "       'Port Phillip City Council','Boroondara City Council', 'Stonnington City Council', 'Glen Eira City Council',\\\n",
    "       'Bayside City Council', 'Banyule City Council', 'Brimbank City Council', 'Casey City Council',\\\n",
    "       'Greater Dandenong City Council', 'Darebin City Council', 'Frankston City Council', 'Hume City Council',\\\n",
    "       'Kingston City Council', 'Knox City Council', 'Manningham City Council', 'Maroondah City Council',\\\n",
    "       'Melton City Council', 'Monash City Council', 'Moonee Valley City Council', 'Moreland City Council',\\\n",
    "       'Whitehorse City Council', 'Whittlesea City Council', 'Wyndham City Council', 'Cardinia Shire Council',\\\n",
    "       'Mornington Peninsula Shire Council', 'Nillumbik Shire Council', 'Yarra Ranges Shire Council']\n",
    "# filter the locality data to just the 31 LGA\n",
    "filtered_locality = locality[locality['Municipality\\r\\nName'].isin(LGA)]\n",
    "# remove duplicates\n",
    "filtered_locality = filtered_locality.drop_duplicates(keep='first').sort_values(['Post\\r\\nCode'], ascending=True)\n",
    "# filter the lat_lon to just the locality data\n",
    "postcodes = filtered_locality['Post\\r\\nCode']\n",
    "filtered_lat_lon = lat_lon[lat_lon['postcode'].isin(postcodes)]\n",
    "\n",
    "# create lat and long columns in filtered_locality\n",
    "filtered_locality['latitude'] = None\n",
    "filtered_locality['longitude'] = None\n",
    "# compate both filtered_locality and filterd_lat_lon to get the lat and lon\n",
    "for i in range(0,len(filtered_locality)):\n",
    "    for j in range(0,len(filtered_lat_lon)):\n",
    "        if (filtered_lat_lon.iloc[j,1] == filtered_locality.iloc[i,0].upper()):\n",
    "            filtered_locality.iloc[i,3] = filtered_lat_lon.iloc[j,2]\n",
    "            filtered_locality.iloc[i,4] = filtered_lat_lon.iloc[j,3]\n",
    "            break;\n",
    "# remove empty rows without latitude and longitude\n",
    "filtered_locality = filtered_locality.dropna(how='any')\n",
    "\n",
    "# output to csv for further use\n",
    "filtered_locality.to_csv('./filtered_locality.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Extracting past 28 days events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text analysis to categorise the events based on different races\n",
    "# import tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# regex of all alphabets of length greater than 3 since race_list shortest word length is 3\n",
    "tokenizer = RegexpTokenizer(r\"\\b[a-zA-Z]{3,}\\b\")\n",
    "\n",
    "# set the 29 race lists based on the census data\n",
    "race_list = ['australian','chinese','croatian','dutch','english','filipino','french','german','greek','hungarian'\\\n",
    "             ,'indian','irish', 'italian', 'korean', 'lebanese', 'macedonian', 'maltese', 'maori', 'new zealander'\\\n",
    "             ,'polish','russian','scottish','serbian','african','spanish','sri lankan','turkish','vietnamese'\\\n",
    "             ,'welsh']\n",
    "\n",
    "# stem the race_list\n",
    "ps = PorterStemmer()\n",
    "for i in range(0,len(race_list)):\n",
    "    race_list[i] = ps.stem(race_list[i])\n",
    "\n",
    "# function to get the culture type of the events\n",
    "def get_culture(event):\n",
    "    # initialise temp output\n",
    "    temp_output = None\n",
    "    # extract name and description for each event and replace the newline and tabs\n",
    "    name = event['name']['text']\n",
    "    desc = event['description']['text']\n",
    "    if desc!=None:\n",
    "        name = name + ' ' + desc\n",
    "    name = name.replace(\"\\n\",\" \").replace(\"\\t\",\" \").strip().lower()\n",
    "    # tokenize each description text into individual word (unigram)\n",
    "    unigrams = tokenizer.tokenize(name)\n",
    "    # stem the unigrams\n",
    "    for x in range(0,len(unigrams)):\n",
    "        unigrams[x] = ps.stem(unigrams[x])\n",
    "    # tokenize each description text into bigrams\n",
    "    bigrams = list(ngrams(unigrams, 2))\n",
    "    # combine both unigrams and bigrams into single list, tokens\n",
    "    tokens = []\n",
    "    for bigram in bigrams:\n",
    "        tokens.append((''.join([w + ' ' for w in bigram])).strip())\n",
    "    tokens = tokens + unigrams\n",
    "    # compare race_list and see if any of the tokens contain them\n",
    "    for race in race_list:\n",
    "        if race in tokens:\n",
    "            temp_output = race\n",
    "    # set race as non-classifiable if no race type matches\n",
    "    if temp_output == None:\n",
    "        temp_output = 'non-classifiable'\n",
    "    return temp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# import eventbrite and requests library for api requests\n",
    "from eventbrite import Eventbrite\n",
    "import requests\n",
    "# import json for data storage and printing\n",
    "import json\n",
    "\n",
    "# empty dataframe to store final result\n",
    "past_events = pd.DataFrame(columns=['LGA','name','desc','id','start_date','category'])\n",
    "# loop through the 31 LGA\n",
    "for region in LGA:\n",
    "    # filter locality data to each individual LGA\n",
    "    temp_locality=filtered_locality[filtered_locality.iloc[:,2]==region]\n",
    "    # create a temp dataframe for this region\n",
    "    temp = pd.DataFrame(columns=['LGA','name','desc','id','start_date','category'])\n",
    "    # loop through each suburb in this region\n",
    "    for i in range(0, len(temp_locality)):\n",
    "        # eventbrite api access code\n",
    "        headers = {\n",
    "        'Authorization': 'Bearer xxxxxxxxxxxxxxxxxxxx',  ############# INPUT EVENTBRITE API ACCESS CODE HERE ############\n",
    "        }\n",
    "        # set params with latitude, longitude, radius of 2km, start date and end date defined at the start\n",
    "        params = (\n",
    "            ('location.longitude', temp_locality.iloc[i,4]),\n",
    "            ('location.latitude', temp_locality.iloc[i,3]),\n",
    "            ('location.within', '2km'),\n",
    "            ('start_date.range_start', start_date),\n",
    "            ('start_date.range_end', end_date)\n",
    "        )\n",
    "        # get response from eventbrite api\n",
    "        response = requests.get('https://www.eventbriteapi.com/v3/events/search', headers=headers, params=params)\n",
    "        # store data in json format\n",
    "        json_data = json.loads(response.text)\n",
    "        # extract just the 'events'\n",
    "        overall_data = json_data['events']\n",
    "        # since each request will only show 50 events per page, if there are instance >50 events, run this loop\n",
    "        if json_data['pagination']['page_count'] > 1:\n",
    "            for j in range (2, json_data['pagination']['page_count']+1):\n",
    "                params = (\n",
    "                    ('location.longitude', temp_locality.iloc[i,4]),\n",
    "                    ('location.latitude', temp_locality.iloc[i,3]),\n",
    "                    ('location.within', '2km'),\n",
    "                    ('start_date.range_start', start_date),\n",
    "                    ('start_date.range_end', end_date),\n",
    "                    ('page', j)\n",
    "                )\n",
    "                # send request to eventbrite api again to get the next 50 events and so on\n",
    "                response = requests.get('https://www.eventbriteapi.com/v3/events/search', headers=headers, params=params)\n",
    "                json_data = json.loads(response.text)\n",
    "                # append all events into overall_data list\n",
    "                for each in json_data['events']:\n",
    "                    overall_data.append(each)\n",
    "        # loop through eventbrite data and extract only 'name','desc','id' columns\n",
    "        for k in range(0, len(overall_data)):\n",
    "            # run get_culture function to get the cultural category for this event\n",
    "            category = get_culture(overall_data[k])\n",
    "            temp.loc[len(temp)] = [region,overall_data[k]['name']['text'],overall_data[k]['description']['text'],\\\n",
    "                                   overall_data[k]['id'],overall_data[k]['start']['local'],category]\n",
    "        # remove duplicated rows based on 'id'\n",
    "        temp = temp.drop_duplicates(subset='id', keep=\"first\")\n",
    "        # get 5 most recent events\n",
    "        temp = temp.sort_values('start_date',ascending=False)\n",
    "        temp = temp.iloc[0:5,:]\n",
    "    # append/concat this temp dataframe for this region to final dataframe\n",
    "    past_events = pd.concat([past_events,temp])\n",
    "    \n",
    "# output to csv file for further use\n",
    "past_events.to_csv('./eventbritedatapastevents.csv', index=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Extracting future events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# empty dataframe to store final result\n",
    "future_events = pd.DataFrame(columns=['LGA','name','id','category'])\n",
    "# loop through the 31 LGA\n",
    "for region in LGA:\n",
    "    # filter locality data to each individual LGA\n",
    "    temp_locality=filtered_locality[filtered_locality.iloc[:,2]==region]\n",
    "    # create a temp dataframe for this region\n",
    "    temp = pd.DataFrame(columns=['LGA','name','id','category'])\n",
    "    # loop through each suburb in this region\n",
    "    for i in range(0, len(temp_locality)):\n",
    "        # eventbrite api access code\n",
    "        headers = {\n",
    "        'Authorization': 'Bearer xxxxxxxxxxxxxxxxxxxx',  ############# INPUT EVENTBRITE API ACCESS CODE HERE ############\n",
    "        }\n",
    "        # set params with latitude, longitude, radius of 2km, start date and end date Aug 1 to Aug 31\n",
    "        params = (\n",
    "            ('location.longitude', temp_locality.iloc[i,4]),\n",
    "            ('location.latitude', temp_locality.iloc[i,3]),\n",
    "            ('location.within', '2km'),\n",
    "            ('start_date.range_start', end_date)\n",
    "        )\n",
    "        # get response from eventbrite api\n",
    "        response = requests.get('https://www.eventbriteapi.com/v3/events/search', headers=headers, params=params)\n",
    "        # store data in json format\n",
    "        json_data = json.loads(response.text)\n",
    "        # extract just the 'events'\n",
    "        overall_data = json_data['events']\n",
    "        # since each request will only show 50 events per page, if there are instance >50 events, run this loop\n",
    "        if json_data['pagination']['page_count'] > 1:\n",
    "            for j in range (2, json_data['pagination']['page_count']+1):\n",
    "                params = (\n",
    "                    ('location.longitude', temp_locality.iloc[i,4]),\n",
    "                    ('location.latitude', temp_locality.iloc[i,3]),\n",
    "                    ('location.within', '2km'),\n",
    "                    ('start_date.range_start', end_date),\n",
    "                    ('page', j)\n",
    "                )\n",
    "                # send request to eventbrite api again to get the next 50 events and so on\n",
    "                response = requests.get('https://www.eventbriteapi.com/v3/events/search', headers=headers, params=params)\n",
    "                json_data = json.loads(response.text)\n",
    "                # append all events into overall_data list\n",
    "                for each in json_data['events']:\n",
    "                    overall_data.append(each)\n",
    "        # loop through eventbrite data and extract only 'name','desc','id' columns\n",
    "        for k in range(0, len(overall_data)):\n",
    "            # run get_culture function to get the cultural category for this event\n",
    "            category = get_culture(overall_data[k])\n",
    "            temp.loc[len(temp)] = [region,overall_data[k]['name']['text'],overall_data[k]['id'],category]\n",
    "        # remove duplicated rows based on 'id'\n",
    "        temp = temp.drop_duplicates(subset='id', keep=\"first\")\n",
    "    # append/concat this temp dataframe for this region to final dataframe\n",
    "    future_events = pd.concat([future_events,temp])\n",
    "    \n",
    "# output to csv file for further use\n",
    "future_events.to_csv('./eventbritedatafutureevents.csv', index=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------- END OF SCRIPT --------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
